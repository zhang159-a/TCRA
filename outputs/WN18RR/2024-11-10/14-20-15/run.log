[2024-11-10 14:20:15,613][root][INFO] - 
------Config------
 {'dataset': 'WN18RR', 'epoch': 10, 'batch_size': 32, 'learning_rate': 0.0015, 'h_dim': 200, 'pred_rel_w': True, 'label_smooth': 0.1, 'kg_layer': 2, 'rm_rate': 0.5, 'ent_drop': 0.2, 'rel_drop': 0, 'comp_op': 'mul', 'bn': True, 'k_h': 10, 'k_w': 20, 'ent_drop_pred': 0.3, 'conv_drop': 0.1, 'fc_drop': 0.4, 'ker_sz': 7, 'out_channel': 250, 'model': 'TCRA', 's_dim': 200, 'decoder': 'ConvE', 'device': 0, 'warmup_epoch': 5, 'max_no_improve': 10, 'cpu_worker_num': 6, 'rule_dim': 200, 'rule_hidden_dim': 200, 'rule_heads': 2, 'rule_layers': 4, 'rule_drop': 0.1, 'rule_len': 10, 'rules_num': 3000, 'num_encoder': 2, 'gamma': 12}
[2024-11-10 14:20:15,613][root][INFO] - Code dir path: d:\GitHub\TCRA\code
[2024-11-10 14:20:15,614][root][INFO] - Config dir path: d:\GitHub\TCRA\config
[2024-11-10 14:20:15,614][root][INFO] - Model save path: D:\GitHub\TCRA
[2024-11-10 14:20:16,003][root][INFO] - Number of parameters: 27813352
[2024-11-10 14:20:17,191][root][INFO] - ---Load Rules---
[2024-11-10 14:20:17,200][root][INFO] - # rules: 7382
[2024-11-10 14:20:19,608][root][INFO] - kg # node: 40943
[2024-11-10 14:20:19,608][root][INFO] - kg # edge: 173670
[2024-11-10 14:20:19,609][root][INFO] - kg # zero deg node: 384
[2024-11-10 14:20:19,609][root][INFO] - ---Load Train Data---
[2024-11-10 14:20:20,285][root][INFO] - -----Model Parameter Configuration-----
[2024-11-10 14:20:20,286][root][INFO] - Parameter ent_emb: torch.Size([40943, 200]), require_grad = True
[2024-11-10 14:20:20,286][root][INFO] - Parameter ent_emb1: torch.Size([40943, 200]), require_grad = True
[2024-11-10 14:20:20,286][root][INFO] - Parameter r_rel_emb: torch.Size([23, 200]), require_grad = True
[2024-11-10 14:20:20,286][root][INFO] - Parameter rel_w: torch.Size([400, 200]), require_grad = True
[2024-11-10 14:20:20,286][root][INFO] - Parameter rel_embs.0: torch.Size([23, 200]), require_grad = True
[2024-11-10 14:20:20,287][root][INFO] - Parameter rel_embs.1: torch.Size([23, 200]), require_grad = True
[2024-11-10 14:20:20,287][root][INFO] - Parameter edge_layers.0.neigh_w: torch.Size([200, 200]), require_grad = True
[2024-11-10 14:20:20,287][root][INFO] - Parameter edge_layers.0.bn.weight: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,288][root][INFO] - Parameter edge_layers.0.bn.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,288][root][INFO] - Parameter edge_layers.1.neigh_w: torch.Size([200, 200]), require_grad = True
[2024-11-10 14:20:20,288][root][INFO] - Parameter edge_layers.1.bn.weight: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,288][root][INFO] - Parameter edge_layers.1.bn.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,289][root][INFO] - Parameter node_layers.0.neigh_w: torch.Size([200, 200]), require_grad = True
[2024-11-10 14:20:20,289][root][INFO] - Parameter node_layers.0.bn.weight: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,289][root][INFO] - Parameter node_layers.0.bn.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,289][root][INFO] - Parameter node_layers.1.neigh_w: torch.Size([200, 200]), require_grad = True
[2024-11-10 14:20:20,289][root][INFO] - Parameter node_layers.1.bn.weight: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,290][root][INFO] - Parameter node_layers.1.bn.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,290][root][INFO] - Parameter comp_layers.0.neigh_w: torch.Size([200, 200]), require_grad = True
[2024-11-10 14:20:20,290][root][INFO] - Parameter comp_layers.0.bn.weight: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,290][root][INFO] - Parameter comp_layers.0.bn.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,290][root][INFO] - Parameter comp_layers.1.neigh_w: torch.Size([200, 200]), require_grad = True
[2024-11-10 14:20:20,290][root][INFO] - Parameter comp_layers.1.bn.weight: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,292][root][INFO] - Parameter comp_layers.1.bn.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,292][root][INFO] - Parameter rule_layer.fc_R.weight: torch.Size([200, 200]), require_grad = True
[2024-11-10 14:20:20,292][root][INFO] - Parameter rule_layer.fc_R.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,292][root][INFO] - Parameter rule_layer.fc1.weight: torch.Size([200, 2000]), require_grad = True
[2024-11-10 14:20:20,293][root][INFO] - Parameter rule_layer.fc1.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,293][root][INFO] - Parameter rule_layer.rule_transformer.encoder.attention.fc_Q.weight: torch.Size([200, 200]), require_grad = True
[2024-11-10 14:20:20,293][root][INFO] - Parameter rule_layer.rule_transformer.encoder.attention.fc_Q.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,294][root][INFO] - Parameter rule_layer.rule_transformer.encoder.attention.fc_K.weight: torch.Size([200, 200]), require_grad = True
[2024-11-10 14:20:20,294][root][INFO] - Parameter rule_layer.rule_transformer.encoder.attention.fc_K.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,294][root][INFO] - Parameter rule_layer.rule_transformer.encoder.attention.fc_V.weight: torch.Size([200, 200]), require_grad = True
[2024-11-10 14:20:20,294][root][INFO] - Parameter rule_layer.rule_transformer.encoder.attention.fc_V.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,294][root][INFO] - Parameter rule_layer.rule_transformer.encoder.attention.fc.weight: torch.Size([200, 200]), require_grad = True
[2024-11-10 14:20:20,295][root][INFO] - Parameter rule_layer.rule_transformer.encoder.attention.fc.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,295][root][INFO] - Parameter rule_layer.rule_transformer.encoder.attention.layer_norm.weight: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,295][root][INFO] - Parameter rule_layer.rule_transformer.encoder.attention.layer_norm.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,295][root][INFO] - Parameter rule_layer.rule_transformer.encoder.feed_forward.fc1.weight: torch.Size([200, 200]), require_grad = True
[2024-11-10 14:20:20,296][root][INFO] - Parameter rule_layer.rule_transformer.encoder.feed_forward.fc1.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,296][root][INFO] - Parameter rule_layer.rule_transformer.encoder.feed_forward.fc2.weight: torch.Size([200, 200]), require_grad = True
[2024-11-10 14:20:20,296][root][INFO] - Parameter rule_layer.rule_transformer.encoder.feed_forward.fc2.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,296][root][INFO] - Parameter rule_layer.rule_transformer.encoder.feed_forward.layer_norm.weight: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,296][root][INFO] - Parameter rule_layer.rule_transformer.encoder.feed_forward.layer_norm.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,297][root][INFO] - Parameter rule_layer.rule_transformer.encoders.0.attention.fc_Q.weight: torch.Size([200, 200]), require_grad = True
[2024-11-10 14:20:20,297][root][INFO] - Parameter rule_layer.rule_transformer.encoders.0.attention.fc_Q.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,297][root][INFO] - Parameter rule_layer.rule_transformer.encoders.0.attention.fc_K.weight: torch.Size([200, 200]), require_grad = True
[2024-11-10 14:20:20,298][root][INFO] - Parameter rule_layer.rule_transformer.encoders.0.attention.fc_K.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,298][root][INFO] - Parameter rule_layer.rule_transformer.encoders.0.attention.fc_V.weight: torch.Size([200, 200]), require_grad = True
[2024-11-10 14:20:20,298][root][INFO] - Parameter rule_layer.rule_transformer.encoders.0.attention.fc_V.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,298][root][INFO] - Parameter rule_layer.rule_transformer.encoders.0.attention.fc.weight: torch.Size([200, 200]), require_grad = True
[2024-11-10 14:20:20,299][root][INFO] - Parameter rule_layer.rule_transformer.encoders.0.attention.fc.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,299][root][INFO] - Parameter rule_layer.rule_transformer.encoders.0.attention.layer_norm.weight: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,299][root][INFO] - Parameter rule_layer.rule_transformer.encoders.0.attention.layer_norm.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,299][root][INFO] - Parameter rule_layer.rule_transformer.encoders.0.feed_forward.fc1.weight: torch.Size([200, 200]), require_grad = True
[2024-11-10 14:20:20,299][root][INFO] - Parameter rule_layer.rule_transformer.encoders.0.feed_forward.fc1.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,299][root][INFO] - Parameter rule_layer.rule_transformer.encoders.0.feed_forward.fc2.weight: torch.Size([200, 200]), require_grad = True
[2024-11-10 14:20:20,300][root][INFO] - Parameter rule_layer.rule_transformer.encoders.0.feed_forward.fc2.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,300][root][INFO] - Parameter rule_layer.rule_transformer.encoders.0.feed_forward.layer_norm.weight: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,300][root][INFO] - Parameter rule_layer.rule_transformer.encoders.0.feed_forward.layer_norm.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,300][root][INFO] - Parameter rule_layer.rule_transformer.encoders.1.attention.fc_Q.weight: torch.Size([200, 200]), require_grad = True
[2024-11-10 14:20:20,301][root][INFO] - Parameter rule_layer.rule_transformer.encoders.1.attention.fc_Q.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,301][root][INFO] - Parameter rule_layer.rule_transformer.encoders.1.attention.fc_K.weight: torch.Size([200, 200]), require_grad = True
[2024-11-10 14:20:20,301][root][INFO] - Parameter rule_layer.rule_transformer.encoders.1.attention.fc_K.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,302][root][INFO] - Parameter rule_layer.rule_transformer.encoders.1.attention.fc_V.weight: torch.Size([200, 200]), require_grad = True
[2024-11-10 14:20:20,302][root][INFO] - Parameter rule_layer.rule_transformer.encoders.1.attention.fc_V.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,302][root][INFO] - Parameter rule_layer.rule_transformer.encoders.1.attention.fc.weight: torch.Size([200, 200]), require_grad = True
[2024-11-10 14:20:20,302][root][INFO] - Parameter rule_layer.rule_transformer.encoders.1.attention.fc.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,302][root][INFO] - Parameter rule_layer.rule_transformer.encoders.1.attention.layer_norm.weight: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,303][root][INFO] - Parameter rule_layer.rule_transformer.encoders.1.attention.layer_norm.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,303][root][INFO] - Parameter rule_layer.rule_transformer.encoders.1.feed_forward.fc1.weight: torch.Size([200, 200]), require_grad = True
[2024-11-10 14:20:20,303][root][INFO] - Parameter rule_layer.rule_transformer.encoders.1.feed_forward.fc1.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,304][root][INFO] - Parameter rule_layer.rule_transformer.encoders.1.feed_forward.fc2.weight: torch.Size([200, 200]), require_grad = True
[2024-11-10 14:20:20,304][root][INFO] - Parameter rule_layer.rule_transformer.encoders.1.feed_forward.fc2.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,304][root][INFO] - Parameter rule_layer.rule_transformer.encoders.1.feed_forward.layer_norm.weight: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,304][root][INFO] - Parameter rule_layer.rule_transformer.encoders.1.feed_forward.layer_norm.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,305][root][INFO] - Parameter rule_layer.rule_transformer.fc1.weight: torch.Size([200, 200]), require_grad = True
[2024-11-10 14:20:20,305][root][INFO] - Parameter rule_layer.rule_transformer.fc1.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,305][root][INFO] - Parameter global_layer.s_w: torch.Size([200, 200]), require_grad = True
[2024-11-10 14:20:20,305][root][INFO] - Parameter global_layer.fc_R.weight: torch.Size([200, 200]), require_grad = True
[2024-11-10 14:20:20,306][root][INFO] - Parameter global_layer.fc_R.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,306][root][INFO] - Parameter predictor.bn0.weight: torch.Size([1]), require_grad = True
[2024-11-10 14:20:20,306][root][INFO] - Parameter predictor.bn0.bias: torch.Size([1]), require_grad = True
[2024-11-10 14:20:20,306][root][INFO] - Parameter predictor.bn1.weight: torch.Size([250]), require_grad = True
[2024-11-10 14:20:20,307][root][INFO] - Parameter predictor.bn1.bias: torch.Size([250]), require_grad = True
[2024-11-10 14:20:20,307][root][INFO] - Parameter predictor.bn2.weight: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,307][root][INFO] - Parameter predictor.bn2.bias: torch.Size([200]), require_grad = True
[2024-11-10 14:20:20,307][root][INFO] - Parameter predictor.conv.weight: torch.Size([250, 1, 7, 7]), require_grad = True
[2024-11-10 14:20:20,308][root][INFO] - Parameter predictor.fc.weight: torch.Size([200, 49000]), require_grad = True
[2024-11-10 14:20:20,308][root][INFO] - Training... total epoch: 10, step: 32350
